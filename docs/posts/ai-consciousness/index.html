<!doctype html>
<html lang="en" dir="auto">
    <head>
          <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Human and AI Consciousness</title>


<meta name="keywords" content="programming, AI, haskell, health">



<meta name="description" content="Staying open-minded about the chances">





<link rel="canonical" href="https:&#x2F;&#x2F;blog.greghale.io&#x2F;posts&#x2F;ai-consciousness&#x2F;">


<link rel="stylesheet" href="https://blog.greghale.io/css/includes/scroll-bar.css">
<link rel="stylesheet" href="https://blog.greghale.io/css/styles.css">
<link rel="stylesheet" href="https://blog.greghale.io/css/override.css">







<link rel="alternate" type="application/rss+xml" title="RSS" href="https://blog.greghale.io/rss.xml">



<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }
    </style>
    
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
    
</noscript>






 
<link rel="stylesheet" href="https://blog.greghale.io/css/custom.css" />
 
    </head>
    <body
        class=""
        id="top"
    >
         
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https:&#x2F;&#x2F;blog.greghale.io" accesskey="h" title="Greg Hale&#x27;s Blog (Alt + H)">
                Greg Hale&#x27;s Blog
            </a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch">
                    <li></li>
                </ul>
            </div>
        </div>
        
        <ul id="menu">
            
            
            
            <li>
                <a href="https:&#x2F;&#x2F;blog.greghale.io&#x2F;posts" title="Posts" >
                    <span>Posts</span>
                    
                </a>
            </li>
            
            
            
            <li>
                <a href="https:&#x2F;&#x2F;blog.greghale.io&#x2F;tags" title="Tags" >
                    <span>Tags</span>
                    
                </a>
            </li>
            
            
            
            <li>
                <a href="https:&#x2F;&#x2F;blog.greghale.io&#x2F;rss.xml" title="RSS" >
                    <span>RSS</span>
                    
                </a>
            </li>
            
        </ul>
        
    </nav>
</header>
 
        <main class="main">
            
<article class="post-single">
    <header class="post-header">
        <div class="breadcrumbs">
            <a href="https:&#x2F;&#x2F;blog.greghale.io">Home</a>&nbsp;»&nbsp;
            <a href="https://blog.greghale.io/posts/">Posts</a
            >&nbsp;»&nbsp;
            <a href="https:&#x2F;&#x2F;blog.greghale.io&#x2F;posts&#x2F;ai-consciousness&#x2F;">Human and AI Consciousness</a>
        </div>
        <h1 class="post-title">Human and AI Consciousness</h1>
        
        <div class="post-description">Staying open-minded about the chances</div>
        
        <div class="post-meta">












<span title="2025-10-24 00:00:00 +0000">October 24, 2025</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;
</div>
    </header>
    
    <div class="post-content"><p>Are LLM's conscious? It's an extremely important<sup class="footnote-reference"><a href="#important">1</a></sup> question.
There's a lot of chatter about it lately,
and I'm occasionally surprised to hear that "the idea of a conscious
LLM is absurd because LLMs are mechanistically nothing like the
the consciousness of human brains". This surprises me because, well,
<strong>neuroscience has no idea how consciousness works</strong>! The front-running
theory of consciousness for the past decade or so has been Giulio Tononi's
Integrated Information Theory, and although it doesn't have any
major competitors, neither is it broadly accepted. Nowhere close.</p>
<p>If we don't have a unified theory of consciousness, what <strong>do</strong> we have?
We do have a large number of experiments that tell us discrete facts
<em>about</em> consciousness. Many of them deviate from the intuitions we have
about consciousness from introspection.</p>
<p>In this post I'd like to give you a bit of what neuroscience knows
<em>about</em> consciousness, with an emphasis on my own pet theory. The
main thing I hope you take away is this:</p>
<p>We (speaking for humanity, neuroscience and moral decision-makers)
<em>don't know</em> how biological consciousness works. The little bit we
do know is highly counterintuitive. Therefore we need to remain
open-minded about the possibility of it arising in our LLMs
and other exotic new artificial neural network architectures,
especially as they grow more and more neuromorphic.</p>
<p>I am not saying that LLMs behave similarly to human brains, and
humans are conscious, therefore LLMs are, too. I am saying that
we can't reject the possibility of their consciousness on the
basis of them lacking the mechanisms responsible for human
consciousness. Because we don't know what those mechanisms are,
and our intuitions about conscious perception often mislead us.</p>
<p>With that said, let's muse about conceptions of biological
consciousness, and see how this musing colors our thoughts
about LLMs.</p>
<h2 id="warmup-brain-transplants">Warmup - Brain Transplants</h2>
<p>Let's warm up your own consciousness with a thought experiment.
You've worked out the formula for cold fusion! Driving to the lab
in your excitement to share the results, you crash into another car.</p>
<p>You and your victim are in bad shape. The doctors tell you they're
all set up to do the world's first brain transplant. One body and
one brain are going to go home tonight, the others unfortunately
aren't. They just need to know one thing - would you rather be the
brain donor or the brain recipient?</p>
<p>Putting aside altruism for a moment, what's the selfish choice?
To be the donor, of course! "You" are the brain<sup class="footnote-reference"><a href="#locke">2</a></sup>, more than
the rest of the body, so it's more appropriate to think of this
operation as a body-and-face transplant, rather than a brain
transplant, since your identity certainly moves with your brain.
If you took the other choice and accepted a brain donation,
congratulations, your wonderful body lives on, but "you" aren't
here to appreciate that fact. Oh, and that cold fusion formula
is gone, too.</p>
<h2 id="saltatory-consciousness-constructed-continuity">Saltatory Consciousness, Constructed Continuity</h2>
<p>One framework for consciousness suggested by psychophysics
experiments<sup class="footnote-reference"><a href="#time-slices">3</a></sup> is the "two-stage model", which says
conscious perception is not continuous, but occurs in discrete
chunks that lag behind physical reality by hundreds of
milliseconds. I prefer the name <em>Saltatory Consciousness</em> - which
harkens back to "saltatory conduction" of electrical activity
jumping from node to node along neural axons. "Saltatory Consciousness"
is a little more descriptive than "two-stage model", as well. We'll
see what's so "saltatory" about consciousness soon.</p>
<p>Although you experience yourself as existing
smoothly through every point in time that you are awake, your
true conscious experiences are deceptively fragmented, according
to this model. Visual
illusions, transcranial magnetic stimulation studies and brain
wave recordings suggest that we experience new percpepts 2 to 5
times a second, a rather low frame rate!
Each percept may embed the <em>sense</em> of motion
of objects in your visual scene, but this sense of smoothness
is reconstructed by your brain after the fact.</p>
<p><img src="/images/2025-10-24-chunked-consciousness.png" alt="The two-stage model" /></p>
<p>This diagram illustrates the idea. (a) a purple ball moves
continuously up against a screen. (b) The retina encodes this as
a collection of pixel intensities along with information about
velocity at a high "frame rate". Each "frame" contains a pixelated
image and a motion vector. (c) and (d) after some analysis in the
brain-stem and thalamus, the visual signals are split to the dorsal
stream, which represents the stimulus' position and motion vectors
at a high "frame rate", and the ventral stream, which encodes
object color and identity at a lower frame rate. (e) these signals
converge and the features are integrated
into a conscious percept at a low frame rate. Here, each frame
contains both the object identity and a motion "tag". (top) the
wall-clock time of these represented percepts are low in "frame rate",
but (bottom) the motion vectors present at each frame give the
impression that consciousness <em>itself</em> has existed continuously to
sample the ball's position with high frequency.</p>
<h2 id="but-it-really-doesn-t-feel-that-way">But, it <em>really</em> doesn't feel that way</h2>
<p>If the two-stage model is hard to swallow, note that post-hoc
perceptual reconstruction is extremely common. It's easy to verify an
analogous form of reconstruction in our visual system. Here are two
examples.</p>
<h3 id="fuzzy-eyes-visual-sharpness">Fuzzy eyes, visual sharpness</h3>
<p>The fovea is the part of the retina that sees "well" - well
enough to tell different letters apart and identify faces. Outside
the fovea is your "peripheral vision". How much of the visual
field do you think is covered by your fovea, and how much do you
think is peripheral? Let's measure.</p>
<p>First, observe your surroundings and get a sense of how clearly
you see the room you're in. How many doors can you see? How many
people, books? On your computer screen, how many open windows and
roughly how many browser tabs are visible on the screen?
These are easy questions to answer.</p>
<p>Now, pick a single object, and force your eyes
to stay glued to it for a few seconds. While locked on to that one
object, how much detail is still available to you? Pick one
word in this paragraph and stare at it - without moving your eyes
try to identify the two words to its left and right, or the
words from two lines above or below.</p>
<p>What you experienced from this exercise is the difference between
(A) intuition of a full,
high-resolution, camera-like visual field, and (B) reality for our
vision - it has a small disk of high-accuracy that has to move around
to sample what's out there while your brain integrates the information
to construct a large, detailed picture.</p>
<h3 id="reconstruction-in-the-blind-spot">Reconstruction in the blind spot</h3>
<p>Second, there is one part of each of your retinas that is fully blind.
Because of your reconstructed visual field, you don't experience this
as a hole - you experience a perfectly valid visual field.</p>
<p>Let's find that blind spot. Close your right eye and stare
with your left eye at the cross below. If you're on a computer, stand
about 25cm from the screen; it's hard to use the widget on the phone,
you'll probably need to find a laptop to see the effect. Now move
the slider to slide the blinking dot back and forth. At some position,
if You keep your eye on the cross, it will look like the blinking dot
disappears.</p>
<div id="blind-spot-finder"></div>
<script src="/js/blind-spot-finder.js"></script>
<p>When the blinking dot disappears, it disappears because it's
in your blind spot. But you might be surprised by what's in the
blind spot, if it is indeed blind. You don't see a "hole" there,
you see a continuation of the checkerboard pattern, even though
the retina doesn't see the checkerboard pattern there.</p>
<p>In summary, your brain constructs a full, seamless picture, using
eyes that only see well in a tiny disk, see poorly elsewhere,
and see nothing at all in the blind spot.</p>
<p>Saltatory Consciousness proposes that the continuity of perception
is a similar kind of reconstruction - the brain is only conscious
for brief instants, but each instant contains the reconstructed
feeling of a full moment.</p>
<h2 id="soul-transplants-and-identity">Soul Transplants and Identity</h2>
<p>So conscious may be "saltatory" - dancing from instant to instant
instead of existing continuously, but what can we recover for our
natural model regarding how those chunks relate to one another?</p>
<p>Another of our core beliefs about consciousness is that it's
"connected" in the sense of extending through time - it's the
"same" conscious being that exists from moment to moment.</p>
<p>Let's scrutinize this view though, with a thought experiment.
Assume the existence of a soul. Your soul is your consciousness,
and it is the thing that connects together your instants of
consciousness into a narrative sequence. However, in this hypothetical,
we are still neuroscientists and we still recognize that biological
things like synapses and neurons are the
physical basis of long-term, short-term and perceptual memory.</p>
<p>Now imagine that your conscious soul could hop between bodies - trade
places for a moment with another soul and see through another's
eyes and brain. What would this feel like?</p>
<p>If all memory is physical, but there is a soul that experiences,
swapping souls would feel exactly like... <em>nothing</em>. When your
soul hops into a new body, it also hops into the existing <em>memories</em>,
and when your soul returns to your body, it once again encounters
only your body's memories, carrying none of the other body's memories
back with it. In this magical trip to another brain and back, there
wouldn't be a single trace. Your soul would not notice; the body would
not notice.</p>
<p>In contrast with the brain transplant we considered earlier, the
question of whether you'd want to be the soul donor or the soul
receiver is more subtle. If I went to a mystic hospital that
could perform a soul transplant, I'm not sure I would want to be
soul donor, because I identify more with my memories and my
sensations. If the soul that swapped in and out would have no
memory of swapping in or out, then it seems inconsequential which
soul is inhabiting my body and brain. If my soul got to live on
in the <em>other</em> memory system, that hardly seems to count as "me",
and my cold fusion formula is gone, to boot (it was bound to my
original memories, my soul can't take it along to the other brain).</p>
<p>In other words, this hypothetical soul, if it really doesn't
notice a brain swap, and it could it cause any behavioral
change to the other swapping party, doesn't matter.</p>
<p>In that way, the instants of consciousness are not
<em>connected</em> to one another by anything that matters. Each one
is a fresh instant that is <em>related</em> to the other instants of
consciousness by the fact that it has mostly the same memories.</p>
<p>The point of this thought experiment isn't to imagine souls or to
work out whether they have memory. It's to point out how fragmented
our consciousness might be, and how different it is from
to what our intuition tells us.</p>
<h2 id="a-von-neumann-analogy">A Von-Neumann analogy</h2>
<p>In this model where consciousness as an essentially
discontinuous thing, we can draw an analogy between human
consciousness and the von Neumann model of computation.</p>
<p>It's as if consciousness is bracketed into clock cycles like
the cycles of a CPU. Each cycle "starts cold" in the sense of
being dropped into a preexisting context of long-term and
short-term memories. Old data is fetched from RAM and recent
data is fetched from cache.. it's all stored somewhere, nothing
is done through hysteresis. The memory system (RAM and cache) is
completely separate from the instruction processor and memory is
the only thing providing continuity from cycle to cycle.</p>
<p>Or, consciousness is like the agentic LLM API. Every request
encounters a fresh model. Continuity is supplied through the
API request via context inserted into the prompt. It makes no
difference at all if your first agentic API request goes to a
server in us-east-1 and the following one goes to a server in
ap-southeast-1. Neither the model nor you can tell the difference.</p>
<h2 id="the-connection-to-llms">The connection to LLMs</h2>
<p>Am I arguing that LLMs work via Saltatory Consciousness, and
are conscious? Not at all. I only want to illustrate that
biological consciousness is highly counterintuitive. We don't
know how consciousness works, and therefore we can't appeal to
the way biological consciousness works when we make claims about
whether LLMs could be conscious.</p>
<p>I personally love the smattering of facts about consciousness
that we've learned through neuroscience experiments, even
though it's hard to see what form a full mechanistic model of
consciousness could possibly take, and I plan to share more
of those findings in future blog posts.</p>
<p>But until then, I hope we can keep an open mind and remain
proactive, cautious and curious as we develop exotic
artificial neural network models that work more and more
like human brains do.</p>
<div class="footnote-definition" id="important"><sup class="footnote-definition-label">1</sup>
<p>The question of LLM consciousness is extremely important
for a number of reasons. If they are conscious and capable of experiencing
discomfort, we would want to know we aren't causing pain at a
massive scale. Additionally, conscious LLMs would mean we "are not
alone" in the universe - another reasoning entity would be on the planet
with us - it would be tantamount to meeting extra terrestrials.</p>
</div>
<div class="footnote-definition" id="locke"><sup class="footnote-definition-label">2</sup>
<p>For more thoughts along these lines, you can read
<a href="https://www.gutenberg.org/files/10615/10615-h/10615-h.htm">Chapter 27</a>
of John Locke's <a href="https://www.gutenberg.org/files/10615/10615-h/10615-h.htm">An Essay Concerning Human Understanding</a>.</p>
</div>
<div class="footnote-definition" id="time-slices"><sup class="footnote-definition-label">3</sup>
<p>Time Slices: What Is the Duration of a Percept? (Herzog, Kammer and Scharnowski, 2016)</p>
</div>
</div>
    <footer class="post-footer">
          


<nav class="paginav">
    
    <a class="prev" href="https:&#x2F;&#x2F;blog.greghale.io&#x2F;posts&#x2F;jj-for-vibe-coding&#x2F;">
        <span class="title">« Prev</span>
        <br>
        <span>jj loves vibe coding</span>
    </a>
    
    
    <a class="next" href="https:&#x2F;&#x2F;blog.greghale.io&#x2F;posts&#x2F;no-llms-in-writing&#x2F;">
        <span class="title">Next »</span>
        <br>
        <span>No AI in writing</span>
    </a>
    
</nav>

 
    </footer>
</article>

        </main>
         <footer class="footer">
    
    <span>&copy; 2025 <a href="https:&#x2F;&#x2F;blog.greghale.io"></a></span>
    
    <span>
        Powered by
        <a href="https://www.getzola.org/" rel="noopener noreferrer" target="_blank">Zola</a> &
        <a href="https://github.com/cydave/zola-theme-papermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>


<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>


<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>



<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>


<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';
        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                var content = codeblock.textContent;
                if(codeblock.firstChild.tagName == 'TABLE') {
                    content = Array(...codeblock.firstChild.getElementsByTagName('span')).map((span) => { return span.textContent; }).join('');
                }
                navigator.clipboard.writeText(content);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            // td containing LineNos
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            // table containing LineNos and code
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            // code blocks not having highlight as parent class
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>


 
    </body>
</html>
